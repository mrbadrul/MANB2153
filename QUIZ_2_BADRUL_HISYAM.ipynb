{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QUIZ 2 - BADRUL HISYAM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrbadrul/MANB2153/blob/master/QUIZ_2_BADRUL_HISYAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hJeennUDkD71",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Using the problem and data at https://www.kaggle.com/ronitf/heart-disease-uci,download the data (3KB)\n",
        "\n",
        "2. What type of problem are we trying to solve here? Classification or regression\n",
        "\n",
        " - Classification since the problem is to predict which class a data point is( heart disease present or not)\n",
        "\n",
        "3. You have decided you want to create a neural network model to perform the\n",
        "classification/regression for that problem.\n",
        "\n",
        "\n",
        "a. Use train_test_split function to split the data into training 70%, test 30%\n",
        "\n",
        "b. Design a neural network model with any architecture. Try to be unique\n",
        "here.\n",
        "\n",
        "c. Use Keras codes to create the model\n",
        "\n",
        "d. Compile and fit the model\n",
        "\n",
        "e. What is the accuracy after 20 epochs?\n",
        "\n",
        "\n",
        "- 0.5377"
      ]
    },
    {
      "metadata": {
        "id": "Pqr-x-f-jnyF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "7b69e849-c7ba-4681-ac9b-2233677e696c"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import to_categorical\n",
        "df = pd.read_csv('heart.csv')\n",
        "\n",
        "y = to_categorical(df['target'])\n",
        "x=df.drop(['target'],axis=1)\n",
        "x_train,x_test, y_train, y_test = train_test_split(x,y, test_size=0.3,random_state=40)\n",
        "\n",
        "model = Sequential()\n",
        "  \n",
        "model.add(Dense(500, activation='relu', input_dim=13))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,epochs=20,batch_size=50)\n",
        "score = model.evaluate(x_test, y_test,batch_size=50)\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 7.5877 - acc: 0.5047\n",
            "Epoch 2/20\n",
            "212/212 [==============================] - 0s 182us/step - loss: 7.4508 - acc: 0.5377\n",
            "Epoch 3/20\n",
            "212/212 [==============================] - 0s 174us/step - loss: 7.4508 - acc: 0.5377\n",
            "Epoch 4/20\n",
            "212/212 [==============================] - 0s 157us/step - loss: 7.4508 - acc: 0.5377\n",
            "Epoch 5/20\n",
            "212/212 [==============================] - 0s 167us/step - loss: 7.4508 - acc: 0.5377\n",
            "Epoch 6/20\n",
            "212/212 [==============================] - 0s 152us/step - loss: 7.4508 - acc: 0.5377\n",
            "Epoch 7/20\n",
            "212/212 [==============================] - 0s 145us/step - loss: 7.4508 - acc: 0.5377\n",
            "Epoch 8/20\n",
            "212/212 [==============================] - 0s 160us/step - loss: 7.4508 - acc: 0.5377\n",
            "Epoch 9/20\n",
            "212/212 [==============================] - 0s 150us/step - loss: 7.4508 - acc: 0.5377\n",
            "Epoch 10/20\n",
            "212/212 [==============================] - 0s 171us/step - loss: 7.4508 - acc: 0.5377\n",
            "Epoch 11/20\n",
            "212/212 [==============================] - 0s 160us/step - loss: 7.4508 - acc: 0.5377\n",
            "Epoch 12/20\n",
            "212/212 [==============================] - 0s 165us/step - loss: 7.4508 - acc: 0.5377\n",
            "Epoch 13/20\n",
            "212/212 [==============================] - 0s 154us/step - loss: 7.4508 - acc: 0.5377\n",
            "Epoch 14/20\n",
            "212/212 [==============================] - 0s 160us/step - loss: 7.4508 - acc: 0.5377\n",
            "Epoch 15/20\n",
            "212/212 [==============================] - 0s 155us/step - loss: 7.4508 - acc: 0.5377\n",
            "Epoch 16/20\n",
            "212/212 [==============================] - 0s 158us/step - loss: 7.4508 - acc: 0.5377\n",
            "Epoch 17/20\n",
            "212/212 [==============================] - 0s 159us/step - loss: 7.4508 - acc: 0.5377\n",
            "Epoch 18/20\n",
            "212/212 [==============================] - 0s 153us/step - loss: 7.4508 - acc: 0.5377\n",
            "Epoch 19/20\n",
            "212/212 [==============================] - 0s 162us/step - loss: 7.4508 - acc: 0.5377\n",
            "Epoch 20/20\n",
            "212/212 [==============================] - 0s 156us/step - loss: 7.4508 - acc: 0.5377\n",
            "91/91 [==============================] - 1s 7ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_UolLNuNnYCR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "4. What is underfitting and overfitting?\n",
        "\n",
        " - Overfitting -  model that training data too well.  the model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data.\n",
        "\n",
        " - Underfitting - model that have not enough training. the model cannot capture the underlying trend of the data\n",
        "\n",
        "\n",
        "5. Explain how you can reduce the overfitting of the model.\n",
        "\n",
        "\n",
        "- use cross validation, Use initial training data to generate multiple mini train-test split hten  use these splits to tune the model\n",
        "\n",
        "- Remove some irrelevent input features\n",
        "\n",
        "- Train with more data , so that the algorithms detect the pattern better\n",
        "\n",
        "- Regularization , To artificially forcing model to be simpler."
      ]
    }
  ]
}